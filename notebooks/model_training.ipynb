{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "99cb32b1-b542-416e-8a3d-144dc48759fd",
   "metadata": {},
   "source": [
    "### Introduction\n",
    "Here is a notebook for training a lightGBM model based on the implementation in https://www.kaggle.com/code/ashishpatel26/light-gbm-demand-forecasting/notebook\n",
    "I removed the EDA part and cleaned up the model training part from the original notebook.\n",
    "Although the modeling practice in the original notebook is questionable, it does produce a quick model for API building. We'll focus on machine learning part in anotehr time.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "4c86b8b4-28a1-426a-a6f5-74cc905b60ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "# load required python packages\n",
    "import os\n",
    "import pandas as pd\n",
    "import lightgbm as lgb\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# assume in the sales_forecast repo directory\n",
    "data_dir = '../ml/data'\n",
    "model_dir = '../ml/models'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "87abe259-232d-4979-bedf-933a73af53db",
   "metadata": {},
   "source": [
    "### Load data for model training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "0cd3c46a-f3b8-400d-8feb-15e31c4a658f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# we only need to load train.csv as it is the only file with labels\n",
    "train_df = pd.read_csv(os.path.join(data_dir, 'train.csv'))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "522b5331-2748-46a2-89d6-917e97bcd803",
   "metadata": {},
   "source": [
    "### Data Processing and Splitting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b5e7f5f2-555a-4a41-9142-73de9e1d6caa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# I cleaned up the original function used for creating time features and splitting data \n",
    "def split_data(train_data):\n",
    "    # convert date string to datetime\n",
    "    train_data['date'] = pd.to_datetime(train_data['date'])\n",
    "    # test_data['date'] = pd.to_datetime(test_data['date'])\n",
    "\n",
    "    # get time features\n",
    "    train_data['month'] = train_data['date'].dt.month\n",
    "    train_data['day'] = train_data['date'].dt.dayofweek\n",
    "    train_data['year'] = train_data['date'].dt.year\n",
    "\n",
    "    # test_data['month'] = test_data['date'].dt.month\n",
    "    # test_data['day'] = test_data['date'].dt.dayofweek\n",
    "    # test_data['year'] = test_data['date'].dt.year\n",
    "\n",
    "    # col = [i for i in test_data.columns if i not in ['date','id']]\n",
    "    col = ['store', 'item', 'month', 'day', 'year']\n",
    "    y = 'sales'\n",
    "    train_x, test_x, train_y, test_y = train_test_split(train_data[col],train_data[y], test_size=0.2, random_state=2018) # random split for time series???\n",
    "    return (train_x, test_x, train_y, test_y,col)\n",
    "\n",
    "# split data\n",
    "train_x, test_x, train_y, test_y, col = split_data(train_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b8e77f86-7a0b-4885-88ba-80442f492289",
   "metadata": {},
   "source": [
    "### Model Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a81b6453-e815-4e6f-b866-135572689a8e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def model(train_x,train_y,test_x,test_y):\n",
    "    # model training parameters\n",
    "    params = {\n",
    "        'nthread': 10,\n",
    "         'max_depth': 9,\n",
    "        'task': 'train',\n",
    "        'boosting_type': 'gbdt',\n",
    "        'objective': 'regression_l1',\n",
    "        'metric': 'mape', # this is abs(a-e)/max(1,a)\n",
    "        'num_leaves': 64,\n",
    "        'learning_rate': 0.2,\n",
    "       'feature_fraction': 0.9,\n",
    "       'bagging_fraction': 0.8,\n",
    "        'bagging_freq': 5,\n",
    "        'lambda_l1': 3.097758978478437,\n",
    "        'lambda_l2': 2.9482537987198496,\n",
    "        'verbose': 1,\n",
    "        'min_child_weight': 6.996211413900573,\n",
    "        'min_split_gain': 0.037310344962162616,\n",
    "        }\n",
    "\n",
    "    # convert data into lightgbm.Dataset\n",
    "    lgb_train = lgb.Dataset(train_x,train_y)\n",
    "    lgb_valid = lgb.Dataset(test_x,test_y)\n",
    "\n",
    "    # model training\n",
    "    model = lgb.train(params, \n",
    "                      lgb_train, \n",
    "                      3000, \n",
    "                      valid_sets=[lgb_train, lgb_valid],\n",
    "                      callbacks=[lgb.early_stopping(stopping_rounds=50),\n",
    "                                 lgb.log_evaluation(50)])\n",
    "\n",
    "    # return trained model\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "c2bd1b12-9843-4bb5-9ab9-2b74ad3fc559",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.003058 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 88\n",
      "[LightGBM] [Info] Number of data points in the train set: 730400, number of used features: 5\n",
      "[LightGBM] [Info] Start training from score 47.000000\n",
      "Training until validation scores don't improve for 50 rounds\n",
      "[50]\ttraining's mape: 0.149378\tvalid_1's mape: 0.150001\n",
      "[100]\ttraining's mape: 0.137493\tvalid_1's mape: 0.138531\n",
      "[150]\ttraining's mape: 0.134397\tvalid_1's mape: 0.13578\n",
      "[200]\ttraining's mape: 0.132948\tvalid_1's mape: 0.13467\n",
      "[250]\ttraining's mape: 0.131924\tvalid_1's mape: 0.134041\n",
      "[300]\ttraining's mape: 0.131156\tvalid_1's mape: 0.133594\n",
      "[350]\ttraining's mape: 0.130671\tvalid_1's mape: 0.133409\n",
      "[400]\ttraining's mape: 0.130251\tvalid_1's mape: 0.133271\n",
      "[450]\ttraining's mape: 0.129901\tvalid_1's mape: 0.133194\n",
      "[500]\ttraining's mape: 0.129557\tvalid_1's mape: 0.133103\n",
      "[550]\ttraining's mape: 0.129296\tvalid_1's mape: 0.133054\n",
      "[600]\ttraining's mape: 0.129009\tvalid_1's mape: 0.132993\n",
      "[650]\ttraining's mape: 0.128806\tvalid_1's mape: 0.132988\n",
      "[700]\ttraining's mape: 0.128608\tvalid_1's mape: 0.132972\n",
      "Early stopping, best iteration is:\n",
      "[660]\ttraining's mape: 0.12874\tvalid_1's mape: 0.132955\n"
     ]
    }
   ],
   "source": [
    "model = model(train_x,train_y,test_x,test_y)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1c9e63f-0f6d-4132-98c3-0f2457db8665",
   "metadata": {},
   "source": [
    "### Save model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "240f9b05-bcee-4ddb-98b9-98ecaee0dcb8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<lightgbm.basic.Booster at 0x137e8c350>"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_name = 'lgbm'\n",
    "model_loc = os.path.join(model_dir, model_name)\n",
    "model.save_model(filename = model_loc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f5dd4f1-85ab-49c6-911c-f47484433d7d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "sales-forecast",
   "language": "python",
   "name": "sales-forecast"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
